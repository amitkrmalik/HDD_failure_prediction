{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:38:00.320783: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-13 17:38:00.324372: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-13 17:38:00.334699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-13 17:38:00.350865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-13 17:38:00.355661: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-13 17:38:00.369112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-13 17:38:02.328500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clear_data(df):\n",
    "    # Drop the date column from the dataframe\n",
    "    if 'data' in df.columns:\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "    if 'model' in df.columns:\n",
    "        df.drop(columns=['model'], inplace=True)\n",
    "    if 'serial_number' in df.columns:\n",
    "        df.drop(columns=['serial_number'], inplace=True)\n",
    "\n",
    "    df_filled = df.fillna(df.mean())\n",
    "\n",
    "    # Keep the original 'failure' column unchanged\n",
    "    df_filled['failure'] = df['failure']\n",
    "\n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset into a pandas dataframe\n",
    "column_list = []\n",
    "column_list.append('date')\n",
    "column_list.append('failure')\n",
    "column_list.append('model')\n",
    "column_list.append('serial_number')\n",
    "column_list.append('smart_5_raw')\n",
    "column_list.append('smart_9_raw')\n",
    "column_list.append('smart_184_raw')\n",
    "column_list.append('smart_187_raw')\n",
    "column_list.append('smart_193_raw')\n",
    "column_list.append('smart_197_raw')\n",
    "column_list.append('smart_198_raw')\n",
    "column_list.append('smart_240_raw')\n",
    "column_list.append('smart_241_raw')\n",
    "column_list.append('smart_242_raw')\n",
    "\n",
    "df = pd.read_parquet('/nobackup/amimalik/bits/dataset/dimensions/4Q/failed_devices_df_all.parquet', columns=column_list)\n",
    "model_df = pd.read_csv('/nobackup/amimalik/bits/dataset/dimensions/4Q/sorted_failures.csv')\n",
    "\n",
    "model_df = model_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_iteration_count(df, count):\n",
    "    serial_number_counts = df['serial_number'].value_counts()\n",
    "\n",
    "    # Filter the serial numbers based on value counts\n",
    "    filtered_serial_numbers = serial_number_counts[serial_number_counts > count].index\n",
    "\n",
    "    # Create a new dataframe with filtered serial numbers\n",
    "    filtered_df = df[df['serial_number'].isin(filtered_serial_numbers)].copy()\n",
    "\n",
    "    # reset index inplace \n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # copy the dataframe to ddf\n",
    "    filtered_df = filtered_df.copy()\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resample_data(df, sample_frequency):\n",
    "    # Group the dataframe by serial number\n",
    "    grouped_df = df.groupby('serial_number')\n",
    "\n",
    "    # Create an empty list to store the resampled dataframes\n",
    "    resampled_dfs = []\n",
    "\n",
    "    # Iterate over each group\n",
    "    for name, group in grouped_df:\n",
    "        # Resample the group to weekly frequency\n",
    "        resampled_group = group.resample(sample_frequency, on='date').max()\n",
    "        # drop the tail row if the last row is not a failure\n",
    "        while resampled_group.tail(1)['failure'].values[0] != 1:\n",
    "            resampled_group = resampled_group[:-1]\n",
    "        # Append the resampled group to the list\n",
    "        resampled_dfs.append(resampled_group)\n",
    "\n",
    "    # Concatenate the list of dataframes into a single dataframe\n",
    "    resampled_data = pd.concat(resampled_dfs)\n",
    "\n",
    "    # Reset the index of the dataframe\n",
    "    # resampled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(ddf):\n",
    "    raw_columns = [col for col in ddf.columns if 'raw' in col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(ddf[raw_columns])\n",
    "    normalized_data = scaler.transform(ddf[raw_columns])\n",
    "    normalized_data = pd.DataFrame(normalized_data, columns=raw_columns)\n",
    "\n",
    "    # Drop the raw columns from the original dataframe and combine the normalized data with the original dataframe\n",
    "    combined_df = pd.concat([normalized_data, ddf.drop(columns=raw_columns)], axis=1)\n",
    "\n",
    "    # rename the columns with raw to normalized\n",
    "    combined_df.columns = combined_df.columns.str.replace('_raw', '_norm')\n",
    "\n",
    "\n",
    "    combined_df.head()\n",
    "\n",
    "    combined_df.rename(columns={'failure':'failure', 'smart_5_norm':'Reallocated_Sectors_Count', 'smart_9_norm':'Power-On_Hours',\n",
    "                        'smart_184_norm':'I/O_Error_Detection_and_Correction','smart_187_norm':'Reported_Uncorrectable_Errors', \n",
    "                        'smart_193_norm':'Load_Unload_Cycle', 'smart_197_norm':'Current_Pending_Sector_Count', 'smart_198_norm':'Offline_Uncorrectable',\n",
    "                        'smart_240_norm':'Head_Flying_Hours', 'smart_241_norm':'Total_LBAs_Written', 'smart_242_norm':'Total_LBAs_Read'}, inplace=True)\n",
    "\n",
    "    ddf = combined_df.copy()\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    # Split the dataset into features (X) and target variable (y)\n",
    "    X = df.drop(columns=['failure'])\n",
    "    y = df['failure']\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Print the shape of the train and test sets\n",
    "    # print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "    train_y = np.array(train_y).reshape(-1, 1)\n",
    "    test_y = np.array(test_y).reshape(-1, 1)\n",
    "\n",
    "    # train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "    # Print the shape of reshaped train and test sets\n",
    "    # print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "    return train_X, train_y, test_X, test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    # plot history\\n    plt.plot(history.history['loss'], label='train')\\n    plt.plot(history.history['val_loss'], label='validation')\\n    # Optional: Plot other metrics by adding similar lines\\n    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\\n    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\\n\\n    plt.title('model accuracy for model: ' + dev_model)\\n    plt.ylabel('accuracy')\\n    plt.xlabel('epoch')\\n    plt.legend()\\n    plt.grid()\\n    plt.show()\\n\\n    return model, binary_predictions, scores\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_fit(X_train, y_train, x_test, y_test, dev_model, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    validation_number = int(0.2 * X_train.shape[0])\n",
    "    validation_X = X_train[validation_number:]\n",
    "    validation_y = y_train[validation_number:]\n",
    "    train_X = X_train[:validation_number]\n",
    "    train_y = y_train[:validation_number]\n",
    "\n",
    "    # Calculate class weights using inverse frequency\n",
    "    class_weights = {0: 0.5128462099125365, 1: 19.96099290780142}\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True, dropout=0.25))\n",
    "    model.add(LSTM(64, return_sequences=True, dropout=0.25))\n",
    "    model.add(LSTM(32, return_sequences=False, dropout=0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    # history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(validation_X, validation_y), verbose=0)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(validation_X, validation_y), class_weight=class_weights, verbose=0)\n",
    "\n",
    "    # summarize performance of the model\n",
    "    scores = model.evaluate(train_X, train_y, verbose=0)\n",
    "\n",
    "    # Use the trained model to predict on the test_X dataset\n",
    "    predictions = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # Convert the predictions to binary values (0 or 1)\n",
    "    binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    accuracy = accuracy_score(test_y, binary_predictions)\n",
    "    precision = precision_score(test_y, binary_predictions)\n",
    "    recall = recall_score(test_y, binary_predictions)\n",
    "    f1 = f1_score(test_y, binary_predictions)\n",
    "    roc_auc = roc_auc_score(test_y, binary_predictions)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "    print(\"ROC AUC score:\", roc_auc)\n",
    "\n",
    "    # plot confusion matrix\n",
    "    cm = confusion_matrix(test_y, binary_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    cm\n",
    "\n",
    "    return model, binary_predictions, scores\n",
    "\"\"\"\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    # Optional: Plot other metrics by adding similar lines\n",
    "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "    plt.title('model accuracy for model: ' + dev_model)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return model, binary_predictions, scores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ws/amimalik-bgl/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m19,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,673</span> (252.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,673\u001b[0m (252.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,673</span> (252.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,673\u001b[0m (252.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "Accuracy: 0.9289772727272727\n",
      "Precision: 0.075\n",
      "Recall: 0.1875\n",
      "F1-score: 0.10714285714285714\n",
      "ROC AUC score: 0.5668604651162791\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe to include only serial numbers with more than count records\n",
    "ddf = get_df_with_iteration_count(df, 340)\n",
    "ddf_norm = get_norm(ddf)\n",
    "\n",
    "# Resample the dataframe to weekly frequency\n",
    "weekly_df = get_resample_data(ddf_norm, 'W')\n",
    "\n",
    "# Drop nan values from the dataframe\n",
    "weekly_df = weekly_df.dropna(how='all')\n",
    "\n",
    "# Get the clear data\n",
    "weekly_df = get_clear_data(weekly_df)\n",
    "\n",
    "# Convert the failure column to integer type\n",
    "weekly_df['failure'] = weekly_df['failure'].astype(int)\n",
    "\n",
    "\n",
    "train_X, train_y, test_X, test_y = split_dataset(weekly_df)\n",
    "\n",
    "model_x, predict_y, score =  model_fit(train_X, train_y, test_X, test_y, epochs=200, batch_size=32, learning_rate=0.005, dev_model=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1302,   74],\n",
       "       [  26,    6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot confusion_matrix for model_x\n",
    "cm = confusion_matrix(test_y, predict_y)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x, predict_y, score =  model_fit(train_X, train_y, test_X, test_y, epochs=200, batch_size=32, learning_rate=0.005, dev_model=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the dataframe to include only serial numbers with more than count records\n",
    "ddf = get_df_with_iteration_count(df, 340)\n",
    "ddf_norm = get_norm(ddf)\n",
    "\n",
    "# Resample the dataframe to weekly frequency\n",
    "weekly_df = get_resample_data(ddf_norm, '2W')\n",
    "\n",
    "# Drop nan values from the dataframe\n",
    "weekly_df = weekly_df.dropna(how='all')\n",
    "\n",
    "# Get the clear data\n",
    "weekly_df = get_clear_data(weekly_df)\n",
    "\n",
    "# Convert the failure column to integer type\n",
    "weekly_df['failure'] = weekly_df['failure'].astype(int)\n",
    "\n",
    "\n",
    "train_X, train_y, test_X, test_y = split_dataset(weekly_df)\n",
    "\n",
    "model_x, predict_y, score =  model_fit(train_X, train_y, test_X, test_y, epochs=500, batch_size=32, learning_rate=0.001, dev_model=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the dataframe to include only serial numbers with more than count records\n",
    "ddf = get_df_with_iteration_count(df, 340)\n",
    "ddf_norm = get_norm(ddf)\n",
    "\n",
    "# Resample the dataframe to weekly frequency\n",
    "weekly_df = get_resample_data(ddf_norm, '4W')\n",
    "\n",
    "# Drop nan values from the dataframe\n",
    "weekly_df = weekly_df.dropna(how='all')\n",
    "\n",
    "# Get the clear data\n",
    "weekly_df = get_clear_data(weekly_df)\n",
    "\n",
    "# Convert the failure column to integer type\n",
    "weekly_df['failure'] = weekly_df['failure'].astype(int)\n",
    "\n",
    "\n",
    "train_X, train_y, test_X, test_y = split_dataset(weekly_df)\n",
    "\n",
    "model_x, predict_y, score =  model_fit(train_X, train_y, test_X, test_y, epochs=500, batch_size=32, learning_rate=0.001, dev_model=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_model(model_x, show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
